{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le client chroma_db en précisant le sous-répertoire de la base\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beranger\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'genai' created and all text files embedded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding function (e.g., OpenAI embedding function)\n",
    "# Define a local embedding function using sentence-transformers\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a new collection in the Chroma DB\n",
    "collection_name = \"genai\"\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name, embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Directory containing text files\n",
    "directory_path = \"data\"\n",
    "\n",
    "# Scan the directory and process each text file\n",
    "for file_name in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "        # Create a unique ID for the text data using its hash\n",
    "        file_id = hashlib.md5(file_name.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "        # Add the text data to the collection\n",
    "        collection.add(\n",
    "            documents=[text_data], metadatas=[{\"source\": file_path}], ids=[file_id]\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Collection '{collection_name}' created and all text files embedded successfully.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de collections : 1\n",
      "Collection 'genai' - Nombre de documents : 1\n",
      "  IDs: 291aad172699\n",
      "  Metadata:\n",
      "    Document 1: {'source': 'data\\\\paul_graham_essay.txt'}\n",
      "  Premier document (aperçu): \n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed ...\n",
      "  Embeddings:\n",
      "    Document 1: [-0.03938635  0.03998332 -0.01110203 -0.00434901 -0.06904357 -0.08148401\n",
      " -0.04976932  0.05688301 -0.03658195  0.01380224]...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lister les collections présentes\n",
    "collections = client.list_collections()\n",
    "print(\"Nombre de collections :\", len(collections))\n",
    "\n",
    "# Afficher le nombre de documents par collection\n",
    "for col_name in collections:\n",
    "    col_obj = client.get_collection(col_name)\n",
    "    data = col_obj.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
    "    doc_count = len(data.get(\"documents\", []))\n",
    "    print(f\"Collection '{col_name}' - Nombre de documents : {doc_count}\")\n",
    "\n",
    "    # Display IDs in the collection\n",
    "    ids = data.get(\"ids\", [])\n",
    "    print(f\"  IDs: {', '.join(ids)}\")\n",
    "\n",
    "    # Display metadata if available\n",
    "    metadatas = data.get(\"metadatas\")\n",
    "    if metadatas:\n",
    "        print(\"  Metadata:\")\n",
    "        for i, metadata in enumerate(metadatas):\n",
    "            print(f\"    Document {i + 1}: {metadata}\")\n",
    "\n",
    "    # Show preview of first document (truncated)\n",
    "    documents = data.get(\"documents\", [])\n",
    "    if documents and len(documents) > 0:\n",
    "        preview = (\n",
    "            documents[0][:200] + \"...\" if len(documents[0]) > 200 else documents[0]\n",
    "        )\n",
    "        print(f\"  Premier document (aperçu): {preview}\")\n",
    "\n",
    "    # Display embeddings if available\n",
    "    embeddings = data.get(\"embeddings\")\n",
    "    if embeddings is not None and len(embeddings) > 0:\n",
    "        print(\"  Embeddings:\")\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            print(\n",
    "                f\"    Document {i + 1}: {embedding[:10]}...\"\n",
    "            )  # Truncate for readability\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install LlamaIndex if not already installed\n",
    "# !pip install llama-index llama-index-vector-stores-chroma\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, Document, Settings\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# Use the same embedding model as before for consistency\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Connect to existing Chroma collection\n",
    "chroma_store = ChromaVectorStore.from_existing_collection(\n",
    "    client=client,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# Create a vector index from the Chroma store\n",
    "vector_index = VectorStoreIndex.from_vector_store(chroma_store)\n",
    "\n",
    "# Create a simplified search function\n",
    "def search_documents(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Search the collection for documents similar to the query\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with content and metadata\n",
    "    \"\"\"\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=top_k)\n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example search\n",
    "query = \"startup technology\"\n",
    "print(f\"\\nSearching for: '{query}'\")\n",
    "results = search_documents(query)\n",
    "\n",
    "print(f\"Results: {results}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for node in results.source_nodes:\n",
    "    print(f\"\\nScore: {node.score:.4f}\")\n",
    "    print(f\"Source: {node.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Preview: {node.text[:200]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
